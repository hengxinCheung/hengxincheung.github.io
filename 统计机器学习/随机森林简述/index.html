<!-- build time:Tue Jun 16 2020 00:41:18 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html lang="zh"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no,minimal-ui"><meta name="renderer" content="webkit"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=no,email=no,adress=no"><meta name="theme-color" content="#000000"><meta http-equiv="window-target" content="_top"><title>随机森林简述 | hengxincheung&#39;s Blog</title><meta name="description" content="摘要摘要：随机森林是一种灵活的、便于使用的统计学习方法，即使在没有超参数调整的情况下，往往也能取得好的结果。随机森林是典型的集成学习例子，采用自举法生成多个样本集并分别进行建模，然后组合多颗决策树的预测并投票得出最终结果。随机森林具有较高的预测准确率，且对异常值和噪声不敏感，不容易出现过拟合，在医学、生物信息等领域有着广泛的应用。关键字：随机森林; 集成学习; 自举; 决策树 1 前言传统的分类"><meta property="og:type" content="article"><meta property="og:title" content="随机森林简述"><meta property="og:url" content="https://hengxincheung.github.io/%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%80%E8%BF%B0/"><meta property="og:site_name" content="hengxincheung&#39;s blog"><meta property="og:description" content="摘要摘要：随机森林是一种灵活的、便于使用的统计学习方法，即使在没有超参数调整的情况下，往往也能取得好的结果。随机森林是典型的集成学习例子，采用自举法生成多个样本集并分别进行建模，然后组合多颗决策树的预测并投票得出最终结果。随机森林具有较高的预测准确率，且对异常值和噪声不敏感，不容易出现过拟合，在医学、生物信息等领域有着广泛的应用。关键字：随机森林; 集成学习; 自举; 决策树 1 前言传统的分类"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://raw.githubusercontent.com/hengxinCheung/ImageBed/master/images/%E5%86%B3%E7%AD%96%E6%A0%91%E7%A4%BA%E4%BE%8B%E5%9B%BE.png"><meta property="og:image" content="https://raw.githubusercontent.com/hengxinCheung/ImageBed/master/images/182310220134010.png"><meta property="og:image" content="https://note.youdao.com/yws/api/personal/file/2564C56395A84B8B9754C2AE503CEF95?method=download&amp;shareKey=8b554a6b2605c8e1efb19e259e852d13"><meta property="article:published_time" content="2020-05-26T06:53:11.000Z"><meta property="article:modified_time" content="2020-05-27T07:22:46.853Z"><meta property="article:author" content="hengxincheung"><meta property="article:tag" content="统计机器学习"><meta property="article:tag" content="随机森林"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://raw.githubusercontent.com/hengxinCheung/ImageBed/master/images/%E5%86%B3%E7%AD%96%E6%A0%91%E7%A4%BA%E4%BE%8B%E5%9B%BE.png"><link rel="canonical" href="https://hengxincheung.github.io/%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%80%E8%BF%B0/index.html"><link rel="alternate" href="/atom.xml" title="hengxincheung&#39;s blog" type="application/atom+xml"><link rel="icon" href="/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link href="//cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.css" rel="stylesheet"><link href="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.3.5/dist/jquery.fancybox.min.css" rel="stylesheet"><meta name="generator" content="Hexo 4.2.1"></head><body class="main-center" itemscope itemtype="http://schema.org/WebPage"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="slimContent"><div class="navbar-header"><div class="profile-block text-center"><a id="avatar" href="https://github.com/hengxincheung" target="_blank"><img class="img-circle img-rotate" src="/images/avatar.jpg" width="200" height="200"></a><h2 id="name" class="hidden-xs hidden-sm">hengxincheung</h2><h3 id="title" class="hidden-xs hidden-sm hidden-md">码农</h3><small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Guangzhou, China</small></div><div class="search" id="search-form-wrap"><form class="search-form sidebar-form"><div class="input-group"><input type="text" class="search-form-input form-control" placeholder="搜索"> <span class="input-group-btn"><button type="submit" class="search-form-submit btn btn-flat" onclick="return!1"><i class="icon icon-search"></i></button></span></div></form><div class="ins-search"><div class="ins-search-mask"></div><div class="ins-search-container"><div class="ins-input-wrapper"><input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech> <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button></div><div class="ins-section-wrapper"><div class="ins-section-container"></div></div></div></div></div><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false"><span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span></button></div><nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation"><ul class="nav navbar-nav main-nav menu-highlight"><li class="menu-item menu-item-home"><a href="/."><i class="icon icon-home-fill"></i> <span class="menu-title">首页</span></a></li><li class="menu-item menu-item-archives"><a href="/archives"><i class="icon icon-archives-fill"></i> <span class="menu-title">归档</span></a></li><li class="menu-item menu-item-categories"><a href="/categories"><i class="icon icon-folder"></i> <span class="menu-title">分类</span></a></li><li class="menu-item menu-item-tags"><a href="/tags"><i class="icon icon-tags"></i> <span class="menu-title">标签</span></a></li><li class="menu-item menu-item-about"><a href="/about"><i class="icon icon-cup-fill"></i> <span class="menu-title">关于</span></a></li></ul><ul class="social-links"><li><a href="https://github.com/hengxincheung" target="_blank" title="Github" data-toggle="tooltip" data-placement="top"><i class="icon icon-github"></i></a></li><li><a href="/null" target="_blank" title="Weibo" data-toggle="tooltip" data-placement="top"><i class="icon icon-weibo"></i></a></li><li><a href="/null" target="_blank" title="Twitter" data-toggle="tooltip" data-placement="top"><i class="icon icon-twitter"></i></a></li><li><a href="/null" target="_blank" title="Behance" data-toggle="tooltip" data-placement="top"><i class="icon icon-behance"></i></a></li><li><a href="/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul></nav></div></header><aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar"><div class="slimContent"><div class="widget"><h3 class="widget-title">公告</h3><div class="widget-body"><div id="board"><div class="content"><p>欢迎您，瓜子和茶都没有。</p></div></div></div></div><div class="widget"><h3 class="widget-title">分类</h3><div class="widget-body"><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Java%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/">Java编程思想</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/git/">git</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/">linux快速入门</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/numpy%E6%95%99%E7%A8%8B/">numpy教程</a><span class="category-list-count">15</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/web/">web</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">统计机器学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/">设计模式</a><span class="category-list-count">1</span></li></ul></div></div><div class="widget"><h3 class="widget-title">标签云</h3><div class="widget-body tagcloud"><a href="/tags/Java/" style="font-size:13.75px">Java</a> <a href="/tags/RESTful/" style="font-size:13px">RESTful</a> <a href="/tags/git/" style="font-size:13px">git</a> <a href="/tags/linux/" style="font-size:13.5px">linux</a> <a href="/tags/numpy/" style="font-size:14px">numpy</a> <a href="/tags/python/" style="font-size:14px">python</a> <a href="/tags/web/" style="font-size:13.25px">web</a> <a href="/tags/%E6%9D%83%E9%99%90%E7%B3%BB%E7%BB%9F/" style="font-size:13px">权限系统</a> <a href="/tags/%E7%94%A8%E6%88%B7%E7%B3%BB%E7%BB%9F/" style="font-size:13px">用户系统</a> <a href="/tags/%E7%99%BB%E5%BD%95/" style="font-size:13px">登录</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size:13px">统计机器学习</a> <a href="/tags/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/" style="font-size:13.75px">编程思想</a> <a href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" style="font-size:13px">设计模式</a> <a href="/tags/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/" style="font-size:13px">随机森林</a></div></div><div class="widget"><h3 class="widget-title">最新文章</h3><div class="widget-body"><ul class="recent-post-list list-unstyled no-thumbnail"><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/Java%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/">Java编程思想</a></p><p class="item-title"><a href="/Java%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/13-%E5%AD%97%E7%AC%A6%E4%B8%B2/" class="title">13-字符串</a></p><p class="item-date"><time datetime="2020-06-15T16:40:26.000Z" itemprop="datePublished">2020-06-16</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/Java%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/">Java编程思想</a></p><p class="item-title"><a href="/Java%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/12-%E5%BC%82%E5%B8%B8/" class="title">12-异常</a></p><p class="item-date"><time datetime="2020-06-15T16:38:07.000Z" itemprop="datePublished">2020-06-16</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/Java%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/">Java编程思想</a></p><p class="item-title"><a href="/Java%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/11-%E6%8C%81%E6%9C%89%E5%AF%B9%E8%B1%A1/" class="title">11-持有对象</a></p><p class="item-date"><time datetime="2020-06-03T10:18:51.000Z" itemprop="datePublished">2020-06-03</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/Java%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/">Java编程思想</a></p><p class="item-title"><a href="/Java%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/10-%E5%86%85%E9%83%A8%E7%B1%BB/" class="title">10-内部类</a></p><p class="item-date"><time datetime="2020-05-30T13:51:09.000Z" itemprop="datePublished">2020-05-30</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/Java%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/">Java编程思想</a></p><p class="item-title"><a href="/Java%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/09-%E6%8E%A5%E5%8F%A3/" class="title">09-接口</a></p><p class="item-date"><time datetime="2020-05-30T10:55:09.000Z" itemprop="datePublished">2020-05-30</time></p></div></li></ul></div></div></div></aside><aside class="sidebar sidebar-toc collapse" id="collapseToc" itemscope itemtype="http://schema.org/WPSideBar"><div class="slimContent"><nav id="toc" class="article-toc"><h3 class="toc-title">文章目录</h3><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#摘要"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-前言"><span class="toc-number">2.</span> <span class="toc-text">1 前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-随机森林相关概念介绍"><span class="toc-number">3.</span> <span class="toc-text">2 随机森林相关概念介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#21-自举法"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 自举法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#211-自举法简介"><span class="toc-number">3.1.1.</span> <span class="toc-text">2.1.1 自举法简介</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#212-自举法的基本思想"><span class="toc-number">3.1.2.</span> <span class="toc-text">2.1.2 自举法的基本思想</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#213-自举法的代码实现"><span class="toc-number">3.1.3.</span> <span class="toc-text">2.1.3 自举法的代码实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#22-集成分类器"><span class="toc-number">3.2.</span> <span class="toc-text">2.2 集成分类器</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#221-boosting"><span class="toc-number">3.2.1.</span> <span class="toc-text">2.2.1 Boosting</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#222-bagging"><span class="toc-number">3.2.2.</span> <span class="toc-text">2.2.2 Bagging</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#23-决策树"><span class="toc-number">3.3.</span> <span class="toc-text">2.3 决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#231-决策树简介"><span class="toc-number">3.3.1.</span> <span class="toc-text">2.3.1 决策树简介</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#232-决策树的基本思想"><span class="toc-number">3.3.2.</span> <span class="toc-text">2.3.2 决策树的基本思想</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#233-决策树的优缺点"><span class="toc-number">3.3.3.</span> <span class="toc-text">2.3.3 决策树的优缺点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#234-决策树的代码实现"><span class="toc-number">3.3.4.</span> <span class="toc-text">2.3.4 决策树的代码实现</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-随机森林"><span class="toc-number">4.</span> <span class="toc-text">3 随机森林</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#31-随机森林的定义"><span class="toc-number">4.1.</span> <span class="toc-text">3.1 随机森林的定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#32-抗过拟合性质"><span class="toc-number">4.2.</span> <span class="toc-text">3.2 抗过拟合性质</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#33-随机森林的优缺点"><span class="toc-number">4.3.</span> <span class="toc-text">3.3 随机森林的优缺点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#34-随机森林的现状"><span class="toc-number">4.4.</span> <span class="toc-text">3.4 随机森林的现状</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#35-随机森林的代码实现"><span class="toc-number">4.5.</span> <span class="toc-text">3.5 随机森林的代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-对比实验"><span class="toc-number">5.</span> <span class="toc-text">4 对比实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#41-实验结果"><span class="toc-number">5.1.</span> <span class="toc-text">4.1 实验结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#42-实验代码"><span class="toc-number">5.2.</span> <span class="toc-text">4.2 实验代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考文献"><span class="toc-number">6.</span> <span class="toc-text">参考文献</span></a></li></ol></nav></div></aside><main class="main" role="main"><div class="content"><article id="post-统计机器学习/随机森林简述" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting"><div class="article-header"><h1 class="article-title" itemprop="name">随机森林简述</h1><div class="article-meta"><span class="article-date"><i class="icon icon-calendar-check"></i> <a href="/%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%80%E8%BF%B0/" class="article-date"><time datetime="2020-05-26T06:53:11.000Z" itemprop="datePublished">2020-05-26</time></a></span> <span class="article-category"><i class="icon icon-folder"></i> <a class="article-category-link" href="/categories/%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">统计机器学习</a></span> <span class="article-tag"><i class="icon icon-tags"></i> <a class="article-tag-link" href="/tags/%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">统计机器学习</a>, <a class="article-tag-link" href="/tags/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/" rel="tag">随机森林</a></span> <span class="article-read hidden-xs"><i class="icon icon-eye-fill" aria-hidden="true"></i> <span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span></span></span> <span class="post-comment"><i class="icon icon-comment"></i> <a href="/%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%80%E8%BF%B0/#comments" class="article-comment-link">评论</a></span> <span class="post-wordcount hidden-xs" itemprop="wordCount">字数统计: 6.7k(字)</span> <span class="post-readcount hidden-xs" itemprop="timeRequired">阅读时长: 27(分)</span></div></div><div class="article-entry marked-body" itemprop="articleBody"><h2 id="摘要"><a class="markdownIt-Anchor" href="#摘要"></a> 摘要</h2><p><strong>摘要</strong>：随机森林是一种灵活的、便于使用的统计学习方法，即使在没有超参数调整的情况下，往往也能取得好的结果。随机森林是典型的集成学习例子，采用自举法生成多个样本集并分别进行建模，然后组合多颗决策树的预测并投票得出最终结果。随机森林具有较高的预测准确率，且对异常值和噪声不敏感，不容易出现过拟合，在医学、生物信息等领域有着广泛的应用。</p><p><strong>关键字</strong>：随机森林; 集成学习; 自举; 决策树</p><h2 id="1-前言"><a class="markdownIt-Anchor" href="#1-前言"></a> 1 前言</h2><p>传统的分类模型往往精度不高，且容易出现过拟合问题。而如支持向量机(Support Vector Machine, SVM)、多层神经网络等模型又很少能提供对数据内部关系的观察能力，但这种透明度对于医学决策支持、质量控制等应用领域是至关重要的。因此，很多学者通过聚集多个简单模型来提高预测精度和模型解释性。这种方法被称为集成(ensemble)或分类器组合(classifier combination)，其核心思想在于利用训练数据构建一组基分类模型(base classifier)，然后通过对每个基分类器模型的预测值进行投票（预测值为离散变量）或取平均值（预测值为连续变量）来输出最终的预测值。</p><p>随机森林（Random Forest，RF）是由Leo Bremian与Adele Cutler于2011年提出的一种以决策树作为弱分类器的集成分类器。传统的CART决策树可以提供对内部关系的观察能力，但对训练集中的小波动十分敏感，而随机森林充分吸收了决策树的优点并最大化消除了其缺点。</p><p>为了生成随机森林，通常需要生成随机变量来控制组合中的每个决策树的生长。装袋算法（Bagging）是早期组合树方法之一，又称自助聚集（bootstrap aggregating），是一种从训练集随机抽取部分样本来生成决策树的方法。同时，Ho关于随机子空间（Random subspace）方法做了许多研究，该方法通过对特征变量随机选取子集来生成每颗决策树，对随机森林的研究起到巨大的推进。</p><p>随机森林利用自举法（Bootstrap）方法从原始样本集中进行重采样，进而得到多个样本集，对每个样本集进行决策树建模，然后组合多棵决策树的结果，最后通过投票得出最终的预测结果。大量的理论和研究都证明了随机森林具有很高的预测准确率，且不容易出现过拟合。总结来说，随机森林是一种自然的非线性建模工具，在数据挖掘、生物医学等领域有重大意义和广泛应用。</p><h2 id="2-随机森林相关概念介绍"><a class="markdownIt-Anchor" href="#2-随机森林相关概念介绍"></a> 2 随机森林相关概念介绍</h2><h3 id="21-自举法"><a class="markdownIt-Anchor" href="#21-自举法"></a> 2.1 自举法</h3><h4 id="211-自举法简介"><a class="markdownIt-Anchor" href="#211-自举法简介"></a> 2.1.1 自举法简介</h4><p>自举法是一种通过抽取多个样本集来估计抽样分布的方法，这些样本集中包含单个随机样本的替换内容。这些重复样本被称为重新采样的样本。每个重新采样的样本集的数量小于等于原始样本的数量。自举法是非参数统计中一种重要的估计统计量，并可进行统计量区间估计的统计方法。并且，自举法还可以用来进行假设检验。</p><h4 id="212-自举法的基本思想"><a class="markdownIt-Anchor" href="#212-自举法的基本思想"></a> 2.1.2 自举法的基本思想</h4><p>自举法的基本思想如下:</p><ul><li>从原始样本集中随机抽取一个样本，将其副本放入新的样本集中，并将该样本放回到原始样本集中；</li><li>重复采样<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathdefault">m</span></span></span></span>次，即可得到一个新的样本集。其中次数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathdefault">m</span></span></span></span>一般要小于原始样本集的数目<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathdefault">n</span></span></span></span>;</li><li>重复上述步骤<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.03148em">k</span></span></span></span>次，即可得到<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.03148em">k</span></span></span></span>个新的样本集，计算这<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.03148em">k</span></span></span></span>个样本集的统计量。</li></ul><h4 id="213-自举法的代码实现"><a class="markdownIt-Anchor" href="#213-自举法的代码实现"></a> 2.1.3 自举法的代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载iris数据集</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">iris = np.hstack((iris.data, iris.target.reshape(<span class="number">-1</span>, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义新的样本集的列表</span></span><br><span class="line">samples_list = []</span><br><span class="line"><span class="comment"># 生成10个新的样本集</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    <span class="comment"># 定义新的样本集</span></span><br><span class="line">    samples = []</span><br><span class="line">    <span class="comment"># 新的样本集中采样100个样本</span></span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        <span class="comment"># 获得一个随机下标</span></span><br><span class="line">        idx = random.randint(<span class="number">0</span>, iris.shape[<span class="number">0</span>]<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># 得到一个样本</span></span><br><span class="line">        sample = iris[idx, :].reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># 添加进入新的样本集中</span></span><br><span class="line">        samples.append(sample)</span><br><span class="line">    samples = np.array(samples)</span><br><span class="line">    print(<span class="string">f"样本集[<span class="subst">&#123;k&#125;</span>]的维度是:<span class="subst">&#123;samples.shape&#125;</span>"</span>)</span><br><span class="line">    <span class="comment"># 将新的样本集添加到列表中</span></span><br><span class="line">    samples_list.append(samples)</span><br><span class="line">samples_list = np.array(samples_list)</span><br></pre></td></tr></table></figure><h3 id="22-集成分类器"><a class="markdownIt-Anchor" href="#22-集成分类器"></a> 2.2 集成分类器</h3><p>以多个弱分类器组合成的分类器通常被称为集成分类器(Ensemble Classifier)。常见的集成分类器包括 Boosting和Bootstrap aggregation（简称Bagging）。</p><h4 id="221-boosting"><a class="markdownIt-Anchor" href="#221-boosting"></a> 2.2.1 Boosting</h4><p>Boosting通常的做法是迭代地重复调用弱分类器学习算法构造一系列弱分类器，然后将它们以加权的方式联合起来形成一个强分类器。迭代是Boosting方法的显著特征。</p><p>Boosting的基本流程如下：（1）在首轮调用弱学习算法时，以均匀分布从训练样本全集中抽取一个训练样本子集，并在此子集上学习生成弱分类器；（2）以后每轮对选取此轮样本子集时对前一轮训练失败的样本给以更大的分布权值，使其在这一轮训练中出现的概率增加，使后续的弱分类器集中对比较难训练的样本进行学习，最后在此子集上学习生成弱分类器；（3）迭代若干次后会得到一系列弱分类器，每个都有一个对应的权值，其权值大小根据该分类器的效果而定。最后的分类器由生成的多个分类器加权联合产生。</p><h4 id="222-bagging"><a class="markdownIt-Anchor" href="#222-bagging"></a> 2.2.2 Bagging</h4><p>Bagging是Leo Breiman在1996年提出的通过组合随机生成的训练集而改进分类的集成算法。与Boosting的迭代做法不同，Bagging以独立同分布选取的训练样本子集训练弱分类器，作为其组成元素的各个弱分类器之间也是平权的。</p><p>许多研究都指出神经网络、CART 树和线性回归中的子集选择都是不稳定的，而Bagging可以正常处理不稳定情形。Bagging通过将独立同分布的弱分类器通过平均集成起来，可以在使偏差保持基本不变的同时降低方差以及弱分类器之间相关值的平均。这样，如果独立同分布的弱分类器的相关值和偏差都较小，则总的分类误差可以降低。</p><p>实践和理论双方面都表明，Bagging可以将一个好的但不稳定的过程向着最优化的方向椎动一大步，它可以相对性的降低对稳定性的要求。</p><h3 id="23-决策树"><a class="markdownIt-Anchor" href="#23-决策树"></a> 2.3 决策树</h3><h4 id="231-决策树简介"><a class="markdownIt-Anchor" href="#231-决策树简介"></a> 2.3.1 决策树简介</h4><p>决策树(decision tree)是一类常见的机器学习方法。一般地，一颗决策树包含一个根节点、若干个内部节点和若个叶子节点，其中叶子节点对应于决策结果，其他每个节点则对应于一个属性测试。每个节点包含的样本集合根据属性测试的结果被划分到子节点中。根节点包含整个样本集，从根节点到每个叶子节点的路径对应了一个判定测试序列。</p><p>决策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树。</p><p>决策树学习算法主要由三部分构成：（1）特征选择；（2）决策树生成；（3）决策树剪枝。</p><p>决策树学习算法最著名的代表是ID3、C45和CART。这些算法主要区别在于分类节点上特征选取的标准不同。常用的划分标准包括信息增益、增益率、基尼指数等。此外学者们还设计了许多其他的准则，然而实验表明划分标准只对决策树的尺寸有较大影响，但对泛化能力的影响有限。</p><p><img src="https://raw.githubusercontent.com/hengxinCheung/ImageBed/master/images/%E5%86%B3%E7%AD%96%E6%A0%91%E7%A4%BA%E4%BE%8B%E5%9B%BE.png" alt="决策树示例图"></p><h4 id="232-决策树的基本思想"><a class="markdownIt-Anchor" href="#232-决策树的基本思想"></a> 2.3.2 决策树的基本思想</h4><p>决策树是一种基本的分类与回归方法，它可以看作是一系列<code>if-then</code>规则的集合，也可以认为是定义在特征空间与类空间的条件概率分布。</p><p>决策树的根节点到叶节点的每一条路径构建一条规则，路径内部节点的特征对应着规则的条件，叶子节点的类则对应着规则的结论。</p><p>决策树的生成流程遵循简单且直观的“分而治之”（divide-and-conquer）策略：</p><ul><li>生成一个树节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi><mi>o</mi><mi>d</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">node</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathdefault">n</span><span class="mord mathdefault">o</span><span class="mord mathdefault">d</span><span class="mord mathdefault">e</span></span></span></span>；</li><li>如果样本集中的样本全属于同一个类别<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.07153em">C</span></span></span></span>，将树节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi><mi>o</mi><mi>d</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">node</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathdefault">n</span><span class="mord mathdefault">o</span><span class="mord mathdefault">d</span><span class="mord mathdefault">e</span></span></span></span>标记为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.07153em">C</span></span></span></span>类叶子节点，返回该树节点；</li><li>如果属性集<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault">A</span></span></span></span>为空集或者样本在属性集<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault">A</span></span></span></span>上取值相同，将树节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi><mi>o</mi><mi>d</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">node</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathdefault">n</span><span class="mord mathdefault">o</span><span class="mord mathdefault">d</span><span class="mord mathdefault">e</span></span></span></span>标记为样本集<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.02778em">D</span></span></span></span>中最多的类，返回该树节点成；</li><li>从属性集<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault">A</span></span></span></span>中选取最优划分属性<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mo>∗</mo></msub></mrow><annotation encoding="application/x-tex">a_*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.175696em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>;</li><li>遍历最优划分属性<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mo>∗</mo></msub></mrow><annotation encoding="application/x-tex">a_*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.175696em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>中的每一个值<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>a</mi><mo>∗</mo><mi>v</mi></msubsup></mrow><annotation encoding="application/x-tex">a_{*}^{v}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.911392em;vertical-align:-.247em"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.664392em"><span style="top:-2.4530000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∗</span></span></span></span><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.03588em">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.247em"><span></span></span></span></span></span></span></span></span></span>，为树节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi><mi>o</mi><mi>d</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">node</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathdefault">n</span><span class="mord mathdefault">o</span><span class="mord mathdefault">d</span><span class="mord mathdefault">e</span></span></span></span>生成一个分支，在样本集<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.02778em">D</span></span></span></span>中选出在<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mo>∗</mo></msub></mrow><annotation encoding="application/x-tex">a_*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.175696em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>上取值为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>a</mi><mo>∗</mo><mi>v</mi></msubsup></mrow><annotation encoding="application/x-tex">a_{*}^{v}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.911392em;vertical-align:-.247em"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.664392em"><span style="top:-2.4530000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∗</span></span></span></span><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.03588em">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.247em"><span></span></span></span></span></span></span></span></span></span>的样本子集<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>D</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">D_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.83333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:-.02778em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>。如果<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>D</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">D_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.83333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:-.02778em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>为空，则将分支节点标记为叶节点，其类别标记为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.02778em">D</span></span></span></span>中最多的类，返回该树节点；否则，以<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">(</mo><msub><mi>D</mi><mi>v</mi></msub><mo separator="true">,</mo><mi>A</mi><mo>∖</mo><msub><mi>a</mi><mo>∗</mo></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(D_v, A \setminus a_*)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:-.02778em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">∖</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.175696em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>作为分支节点，重复上述步骤。</li></ul><p>显然，决策树的关键在于如何选择最优划分属性。一般而言，随着划分过程不断进行，决策树的分支节点所包含的样本尽可能都属于同一类别，即节点的“纯度”（purity）越来越高。</p><h4 id="233-决策树的优缺点"><a class="markdownIt-Anchor" href="#233-决策树的优缺点"></a> 2.3.3 决策树的优缺点</h4><ul><li>决策树的优点:<ul><li>易于理解和解释，甚至比线性回归更直观；</li><li>与人类做决策思考的思维习惯相吻合；</li><li>模型可以通过树的形式进行可视化展示；</li><li>可以直接处理非数值型数据，不需要进行哑变量的转化，甚至可以直接处理含缺失值的数据。</li></ul></li><li>决策树的缺点：<ul><li>对于有大量数值型输入和输出的问题，决策树未必是一个好的选择；</li><li>当数值型变量之间存在许多错综复杂的关系，模型不能很好的处理；</li><li>模型鲁棒性差，某一个节点的小小变化都可能导致整棵树发生变化。</li></ul></li></ul><h4 id="234-决策树的代码实现"><a class="markdownIt-Anchor" href="#234-决策树的代码实现"></a> 2.3.4 决策树的代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">divide_on_feature</span><span class="params">(X, feature_i, threshold)</span>:</span></span><br><span class="line">    <span class="string">""" 基于特征索引上的样本值是否大于给定的阈值进行分割"""</span></span><br><span class="line">    split_func = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(threshold, int) <span class="keyword">or</span> isinstance(threshold, float):</span><br><span class="line">        split_func = <span class="keyword">lambda</span> sample: sample[feature_i] &gt;= threshold</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        split_func = <span class="keyword">lambda</span> sample: sample[feature_i] == threshold</span><br><span class="line"></span><br><span class="line">    X_1 = np.array([sample <span class="keyword">for</span> sample <span class="keyword">in</span> X <span class="keyword">if</span> split_func(sample)])</span><br><span class="line">    X_2 = np.array([sample <span class="keyword">for</span> sample <span class="keyword">in</span> X <span class="keyword">if</span> <span class="keyword">not</span> split_func(sample)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.array([X_1, X_2])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_entropy</span><span class="params">(y)</span>:</span></span><br><span class="line">    <span class="string">""" 计算熵 """</span></span><br><span class="line">    log2 = <span class="keyword">lambda</span> x: math.log(x) / math.log(<span class="number">2</span>)</span><br><span class="line">    unique_labels = np.unique(y)</span><br><span class="line">    entropy = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> label <span class="keyword">in</span> unique_labels:</span><br><span class="line">        count = len(y[y == label])</span><br><span class="line">        p = count / len(y)</span><br><span class="line">        entropy += -p * log2(p)</span><br><span class="line">    <span class="keyword">return</span> entropy</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy_score</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    <span class="string">""" 计算精度"""</span></span><br><span class="line">    accuracy = np.sum(y_true == y_pred, axis=<span class="number">0</span>) / len(y_true)</span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecisionNode</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""决策树节点"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, feature_i=None, threshold=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 value=None, true_branch=None, false_branch=None)</span>:</span></span><br><span class="line">        self.feature_i = feature_i  <span class="comment"># 特征索引</span></span><br><span class="line">        self.threshold = threshold  <span class="comment"># 特征的阈值</span></span><br><span class="line">        self.value = value  <span class="comment"># 叶子节点的值</span></span><br><span class="line">        self.true_branch = true_branch  <span class="comment"># 子树</span></span><br><span class="line">        self.false_branch = false_branch  <span class="comment"># 子树</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecisionTree</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""决策树"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, min_samples_split=<span class="number">2</span>, min_impurity=<span class="number">1e-7</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 max_depth=float<span class="params">(<span class="string">"inf"</span>)</span>, loss=None)</span>:</span></span><br><span class="line">        self.root = <span class="literal">None</span>  <span class="comment"># 根节点</span></span><br><span class="line">        <span class="comment"># 调整拆分的最小样本数</span></span><br><span class="line">        self.min_samples_split = min_samples_split</span><br><span class="line">        <span class="comment"># 调整拆分的最小不纯度</span></span><br><span class="line">        self.min_impurity = min_impurity</span><br><span class="line">        <span class="comment"># 树生长的最大深度</span></span><br><span class="line">        self.max_depth = max_depth</span><br><span class="line">        <span class="comment"># 切割树的方法，gini，方差等</span></span><br><span class="line">        self._impurity_calculation = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># 树节点取值的方法，分类树：选取出现最多次数的值，回归树：取所有值的平均值</span></span><br><span class="line">        self._leaf_value_calculation = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># 标签是不是one-hot编码</span></span><br><span class="line">        self.one_dim = <span class="literal">None</span></span><br><span class="line">        self.loss = loss</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y, loss=None)</span>:</span></span><br><span class="line">        <span class="string">""" 训练决策树 """</span></span><br><span class="line">        self.one_dim = len(np.shape(y)) == <span class="number">1</span></span><br><span class="line">        self.root = self._build_tree(X, y)</span><br><span class="line">        self.loss = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_tree</span><span class="params">(self, X, y, current_depth=<span class="number">0</span>)</span>:</span></span><br><span class="line">        <span class="string">""" 递归地构建决策树"""</span></span><br><span class="line">        largest_impurity = <span class="number">0</span></span><br><span class="line">        best_criteria = <span class="literal">None</span>  <span class="comment"># 特征索引和与之</span></span><br><span class="line">        best_sets = <span class="literal">None</span>  <span class="comment"># 数据集的子集</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 检查y是否扩展</span></span><br><span class="line">        <span class="keyword">if</span> len(np.shape(y)) == <span class="number">1</span>:</span><br><span class="line">            y = np.expand_dims(y, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 把y加到X的最后一列上</span></span><br><span class="line">        Xy = np.concatenate((X, y), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        n_samples, n_features = np.shape(X)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> n_samples &gt;= self.min_samples_split <span class="keyword">and</span> current_depth &lt;= self.max_depth:</span><br><span class="line">            <span class="comment"># 计算每个特征的不纯度</span></span><br><span class="line">            <span class="keyword">for</span> feature_i <span class="keyword">in</span> range(n_features):</span><br><span class="line">                <span class="comment"># 特征i的所有值</span></span><br><span class="line">                feature_values = np.expand_dims(X[:, feature_i], axis=<span class="number">1</span>)</span><br><span class="line">                unique_values = np.unique(feature_values)</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 遍历特征列i的所有唯一值并计算不纯度</span></span><br><span class="line">                <span class="keyword">for</span> threshold <span class="keyword">in</span> unique_values:</span><br><span class="line">                    <span class="comment"># 根据索引特征i处X的特征值是否满足阈值来划分X和y </span></span><br><span class="line">                    Xy1, Xy2 = divide_on_feature(Xy, feature_i, threshold)</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> len(Xy1) &gt; <span class="number">0</span> <span class="keyword">and</span> len(Xy2) &gt; <span class="number">0</span>:</span><br><span class="line">                        <span class="comment"># 选择两个集合的y值</span></span><br><span class="line">                        y1 = Xy1[:, n_features:]</span><br><span class="line">                        y2 = Xy2[:, n_features:]</span><br><span class="line"></span><br><span class="line">                        <span class="comment"># 计算不纯度</span></span><br><span class="line">                        impurity = self._impurity_calculation(y, y1, y2)</span><br><span class="line"></span><br><span class="line">                        <span class="comment"># 如果此阈值导致比先前记录的信息增益更高，则保存阈值和特征索引</span></span><br><span class="line">                        <span class="keyword">if</span> impurity &gt; largest_impurity:</span><br><span class="line">                            largest_impurity = impurity</span><br><span class="line">                            best_criteria = &#123;<span class="string">"feature_i"</span>: feature_i, <span class="string">"threshold"</span>: threshold&#125;</span><br><span class="line">                            best_sets = &#123;</span><br><span class="line">                                <span class="string">"leftX"</span>: Xy1[:, :n_features],  </span><br><span class="line">                                <span class="string">"lefty"</span>: Xy1[:, n_features:], </span><br><span class="line">                                <span class="string">"rightX"</span>: Xy2[:, :n_features],</span><br><span class="line">                                <span class="string">"righty"</span>: Xy2[:, n_features:],</span><br><span class="line">                            &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> largest_impurity &gt; self.min_impurity:</span><br><span class="line">            <span class="comment"># 为左右分支建立子树</span></span><br><span class="line">            true_branch = self._build_tree(best_sets[<span class="string">"leftX"</span>], best_sets[<span class="string">"lefty"</span>], current_depth + <span class="number">1</span>)</span><br><span class="line">            false_branch = self._build_tree(best_sets[<span class="string">"rightX"</span>], best_sets[<span class="string">"righty"</span>], current_depth + <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> DecisionNode(feature_i=best_criteria[<span class="string">"feature_i"</span>], threshold=best_criteria[</span><br><span class="line">                <span class="string">"threshold"</span>], true_branch=true_branch, false_branch=false_branch)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 我们在leaf=&gt;确定价值</span></span><br><span class="line">        leaf_value = self._leaf_value_calculation(y)</span><br><span class="line">        <span class="keyword">return</span> DecisionNode(value=leaf_value)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_value</span><span class="params">(self, x, tree=None)</span>:</span></span><br><span class="line">        <span class="string">""" 递归搜索得到的叶子节点的值"""</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> tree <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            tree = self.root</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果我们有一个值（即在叶子节点上）=&gt;返回值作为预测</span></span><br><span class="line">        <span class="keyword">if</span> tree.value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> tree.value</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 选择要测试的特征</span></span><br><span class="line">        feature_value = x[tree.feature_i]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 确定是跟随左分支还是跟随右分支</span></span><br><span class="line">        branch = tree.false_branch</span><br><span class="line">        <span class="keyword">if</span> isinstance(feature_value, int) <span class="keyword">or</span> isinstance(feature_value, float):</span><br><span class="line">            <span class="keyword">if</span> feature_value &gt;= tree.threshold:</span><br><span class="line">                branch = tree.true_branch</span><br><span class="line">        <span class="keyword">elif</span> feature_value == tree.threshold:</span><br><span class="line">            branch = tree.true_branch</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 测试子树</span></span><br><span class="line">        <span class="keyword">return</span> self.predict_value(x, branch)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" 预测单个样本 """</span></span><br><span class="line">        y_pred = []</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> X:</span><br><span class="line">            y_pred.append(self.predict_value(x))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">print_tree</span><span class="params">(self, tree=None, indent=<span class="string">" "</span>)</span>:</span></span><br><span class="line">        <span class="string">""" 可视化决策树 """</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> tree:</span><br><span class="line">            tree = self.root</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果是叶子节点，打印</span></span><br><span class="line">        <span class="keyword">if</span> tree.value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            print(tree.value)</span><br><span class="line">        <span class="comment"># 往深处寻找</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"%s:%s? "</span> % (tree.feature_i, tree.threshold))</span><br><span class="line">            print(<span class="string">"%sT-&gt;"</span> % (indent), end=<span class="string">""</span>)</span><br><span class="line">            self.print_tree(tree.true_branch, indent + indent)</span><br><span class="line">            print(<span class="string">"%sF-&gt;"</span> % (indent), end=<span class="string">""</span>)</span><br><span class="line">            self.print_tree(tree.false_branch, indent + indent)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClassificationTree</span><span class="params">(DecisionTree)</span>:</span></span><br><span class="line">    <span class="string">"""分类树"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_calculate_information_gain</span><span class="params">(self, y, y1, y2)</span>:</span></span><br><span class="line">        <span class="comment"># 切割树的标准，这里使用的是交叉熵</span></span><br><span class="line">        p = len(y1) / len(y)</span><br><span class="line">        entropy = calculate_entropy(y)</span><br><span class="line">        info_gain = entropy - p * \</span><br><span class="line">                              calculate_entropy(y1) - (<span class="number">1</span> - p) * \</span><br><span class="line">                                                      calculate_entropy(y2)</span><br><span class="line">        <span class="comment"># print("info_gain：", info_gain)</span></span><br><span class="line">        <span class="keyword">return</span> info_gain</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_majority_vote</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        <span class="comment"># 计算子节点值的方法，这里使用的是选取数据集中出现最多的种类</span></span><br><span class="line">        most_common = <span class="literal">None</span></span><br><span class="line">        max_count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> label <span class="keyword">in</span> np.unique(y):</span><br><span class="line">            <span class="comment"># Count number of occurences of samples with label</span></span><br><span class="line">            count = len(y[y == label])</span><br><span class="line">            <span class="keyword">if</span> count &gt; max_count:</span><br><span class="line">                most_common = label</span><br><span class="line">                max_count = count</span><br><span class="line">        <span class="comment"># print("most_common :", most_common)</span></span><br><span class="line">        <span class="keyword">return</span> most_common</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="comment"># 将分类树切割的标准与计算子节点值的方式传回给基类DecisionTree</span></span><br><span class="line">        self._impurity_calculation = self._calculate_information_gain</span><br><span class="line">        self._leaf_value_calculation = self._majority_vote</span><br><span class="line">        super(ClassificationTree, self).fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RegressionTree</span><span class="params">(DecisionTree)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_calculate_variance_reduction</span><span class="params">(self, y, y1, y2)</span>:</span></span><br><span class="line">        <span class="comment"># 切割树的标准，这里使用的是平方残差</span></span><br><span class="line">        var_tot = calculate_variance(y)</span><br><span class="line">        var_1 = calculate_variance(y1)</span><br><span class="line">        var_2 = calculate_variance(y2)</span><br><span class="line">        frac_1 = len(y1) / len(y)</span><br><span class="line">        frac_2 = len(y2) / len(y)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 计算方差减少量 </span></span><br><span class="line">        variance_reduction = var_tot - (frac_1 * var_1 + frac_2 * var_2)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> sum(variance_reduction)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_mean_of_y</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        <span class="comment"># 计算子节点值的方法，这里使用的是取数据集中的平均值</span></span><br><span class="line">        value = np.mean(y, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> value <span class="keyword">if</span> len(value) &gt; <span class="number">1</span> <span class="keyword">else</span> value[<span class="number">0</span>]</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="comment"># 将回归树切割的标准与计算子节点值的方式传回给基类DecisionTree</span></span><br><span class="line">        self._impurity_calculation = self._calculate_variance_reduction</span><br><span class="line">        self._leaf_value_calculation = self._mean_of_y</span><br><span class="line">        super(RegressionTree, self).fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试代码</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载iris数据集</span></span><br><span class="line">data = datasets.load_iris()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line"><span class="comment"># 切割训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.4</span>)</span><br><span class="line"><span class="comment"># 实例化分类树</span></span><br><span class="line">clf = ClassificationTree()</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line"></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Accuracy:"</span>, accuracy)</span><br></pre></td></tr></table></figure><h2 id="3-随机森林"><a class="markdownIt-Anchor" href="#3-随机森林"></a> 3 随机森林</h2><h3 id="31-随机森林的定义"><a class="markdownIt-Anchor" href="#31-随机森林的定义"></a> 3.1 随机森林的定义</h3><p>随机森林是一个由<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.03148em">k</span></span></span></span>个树状分类器<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><msub><mi>θ</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>k</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">h(x, \theta_k), k=1,2,...</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault">h</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-.02778em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathdefault" style="margin-right:.03148em">k</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8388800000000001em;vertical-align:-.19444em"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span></span></span></span>组成的分类器，且每棵树为输入变量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathdefault">x</span></span></span></span>归属于哪个类投出平等的一票。</p><p>这里的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>θ</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\theta_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-.02778em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>是独立同分布的随机向量，它通过在独立同分布的bootstrap集上学习而被独立同分布地确定。在预测的时候，被<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>θ</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\theta_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-.02778em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>确定的第<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.03148em">k</span></span></span></span>棵树被用来对输入向量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathdefault">x</span></span></span></span>做预测。</p><p><img src="https://raw.githubusercontent.com/hengxinCheung/ImageBed/master/images/182310220134010.png" alt="随机森林示意图"></p><h3 id="32-抗过拟合性质"><a class="markdownIt-Anchor" href="#32-抗过拟合性质"></a> 3.2 抗过拟合性质</h3><p>设有一个包括若干未知参数的模型和为便于拟合该模型提供的数据集，拟合过程可以简要定义为通过优化模型参数使得模型尽量与训练数据拟合。如果用一个从训练数据相同群落中独立的抽取验证数据，总会发现模型不能像拟合训练数据一样好的拟合验证数据，这被称为过拟合。通常过拟合的程度可以以泛化误差来表现。</p><p>在随机森林的定义基础上，这里假设训练集<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">{</mo><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{(x,y)\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">{</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathdefault" style="margin-right:.03588em">y</span><span class="mclose">)</span><span class="mclose">}</span></span></span></span>是由随机向量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.07847em">X</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.22222em">Y</span></span></span></span>分布所抽取出来的，那么可以定义边际函数为：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>m</mi><mi>g</mi><mo stretchy="false">(</mo><mi>X</mi><mo separator="true">,</mo><mi>Y</mi><mo stretchy="false">)</mo><mo>=</mo><mi>a</mi><msub><mi>v</mi><mi>k</mi></msub><mi>I</mi><mo stretchy="false">(</mo><msub><mi>h</mi><mi>k</mi></msub><mo>=</mo><mi>y</mi><mo stretchy="false">)</mo><mo>−</mo><mi>m</mi><mi>a</mi><msub><mi>x</mi><mrow><mi>j</mi><mi mathvariant="normal">≠</mi><mi>y</mi></mrow></msub><mi>a</mi><msub><mi>v</mi><mi>k</mi></msub><mi>I</mi><mo stretchy="false">(</mo><msub><mi>h</mi><mi>k</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">mg(X, Y) = av_kI(h_k=y) - max_{j \ne y}av_kI(h_k(x)=j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault">m</span><span class="mord mathdefault" style="margin-right:.03588em">g</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:.07847em">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathdefault" style="margin-right:.22222em">Y</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault">a</span><span class="mord"><span class="mord mathdefault" style="margin-right:.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:.07847em">I</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.03588em">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-.286108em"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361079999999999em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.05724em">j</span><span class="mrel mtight"><span class="mrel mtight"><span class="mord mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.69444em"><span style="top:-2.7em"><span class="pstrut" style="height:2.7em"></span><span class="rlap mtight"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="inner"><span class="mrel mtight"></span></span><span class="fix"></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.19444em"><span></span></span></span></span></span></span><span class="mrel mtight">=</span></span><span class="mord mathdefault mtight" style="margin-right:.03588em">y</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mord mathdefault">a</span><span class="mord"><span class="mord mathdefault" style="margin-right:.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:.07847em">I</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.05724em">j</span><span class="mclose">)</span></span></span></span></span></p><p>其中，函数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>I</mi><mo stretchy="false">(</mo><mo>∙</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">I(\bullet)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.07847em">I</span><span class="mopen">(</span><span class="mord">∙</span><span class="mclose">)</span></span></span></span>表示示性函数，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><msub><mi>v</mi><mi>k</mi></msub><mo stretchy="false">(</mo><mo>∙</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">av_k(\bullet)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault">a</span><span class="mord"><span class="mord mathdefault" style="margin-right:.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">∙</span><span class="mclose">)</span></span></span></span>表示取平均。这里的边际函数表示在<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.07847em">X</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.22222em">Y</span></span></span></span>分布下正确投票数的平均数超出错误投票平均数的值。这个值越大则分类正确的信息越大。这样，泛化误差可以如下式表示：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mi>E</mi><mo>=</mo><msub><mi>P</mi><mrow><mi>x</mi><mo separator="true">,</mo><mi>y</mi></mrow></msub><mo stretchy="false">(</mo><mi>m</mi><mi>g</mi><mo stretchy="false">(</mo><mi>X</mi><mo separator="true">,</mo><mi>Y</mi><mo stretchy="false">)</mo><mo>&lt;</mo><mn>0</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">PE = P_{x,y}(mg(X,Y)&lt;0)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.13889em">P</span><span class="mord mathdefault" style="margin-right:.05764em">E</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139200000000003em"><span style="top:-2.5500000000000003em;margin-left:-.13889em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:.03588em">y</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">m</span><span class="mord mathdefault" style="margin-right:.03588em">g</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:.07847em">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathdefault" style="margin-right:.22222em">Y</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">0</span><span class="mclose">)</span></span></span></span></span></p><p>其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.07847em">X</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.22222em">Y</span></span></span></span>下标表示此泛化误差是在随机变量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.07847em">X</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.22222em">Y</span></span></span></span>分布下取得的。</p><p>应用大数定理，随着树的数目的增加，对于所有树参数序列<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>θ</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>θ</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\theta_1, \theta_2, ...</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.02778em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.02778em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span></span></span></span>，泛化误差<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mi>E</mi></mrow><annotation encoding="application/x-tex">PE</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.13889em">P</span><span class="mord mathdefault" style="margin-right:.05764em">E</span></span></span></span>几乎处处收敛于:</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mrow><mi>X</mi><mo separator="true">,</mo><mi>Y</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>P</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">(</mo><mi>X</mi><mo separator="true">,</mo><mi>θ</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mi>y</mi><mo>−</mo><mi>m</mi><mi>a</mi><msub><mi>x</mi><mrow><mi>j</mi><mi mathvariant="normal">≠</mi><mi>Y</mi></mrow></msub><msub><mi>P</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><mi>j</mi><mo stretchy="false">)</mo><mo>&lt;</mo><mn>0</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P_{X,Y}(P_\theta(h(X,\theta))=y) = y-max_{j \ne Y}P_\theta(h(x, \theta)=j)&lt;0)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.328331em"><span style="top:-2.5500000000000003em;margin-left:-.13889em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.07847em">X</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:.22222em">Y</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-.13889em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">h</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:.07847em">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathdefault" style="margin-right:.02778em">θ</span><span class="mclose">)</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.03588em">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.7777700000000001em;vertical-align:-.19444em"></span><span class="mord mathdefault" style="margin-right:.03588em">y</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-.286108em"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361079999999999em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.05724em">j</span><span class="mrel mtight"><span class="mrel mtight"><span class="mord mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.69444em"><span style="top:-2.7em"><span class="pstrut" style="height:2.7em"></span><span class="rlap mtight"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="inner"><span class="mrel mtight"></span></span><span class="fix"></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.19444em"><span></span></span></span></span></span></span><span class="mrel mtight">=</span></span><span class="mord mathdefault mtight" style="margin-right:.22222em">Y</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-.13889em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">h</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathdefault" style="margin-right:.02778em">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.05724em">j</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">0</span><span class="mclose">)</span></span></span></span></span></p><h3 id="33-随机森林的优缺点"><a class="markdownIt-Anchor" href="#33-随机森林的优缺点"></a> 3.3 随机森林的优缺点</h3><ul><li>优点：<ul><li>可以同时处理离散型和连续型特征，可以用于解决分类和回归问题；</li><li>通过平均决策树的方式，降低了过拟合的风险性，具有抗过拟合的能力；</li><li>具有极强的稳定性，只有在半数以上的基分类器出现差错时才会做出错误的预测；</li><li>能够有效地运行在大型数据集上，且能直接处理高维数据而不需要降维；</li><li>在生成过程中，能够获取到内部生成误差的一种无偏估计。</li></ul></li><li>缺点：<ul><li>在解决回归问题时，不能够做出超越训练集数据范围的预测，这可能导致在某些特定噪声的数据进行建模时出现过度拟合；</li><li>可能有很多相似的决策树，掩盖了真实的结果；</li><li>对于小数据或者低维数据（特征较少的数据），可能不能产生很好的分类；</li><li>计算成本高。</li></ul></li></ul><h3 id="34-随机森林的现状"><a class="markdownIt-Anchor" href="#34-随机森林的现状"></a> 3.4 随机森林的现状</h3><p>在随机森林提出后，有不少学者针对它进行改进和优化。</p><p>Rodriguez等人提出用旋转森林取代随机森林。为了创建旋转森林的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault">L</span></span></span></span>个基本分类器中的一个，特征集被随机的分成<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.07153em">K</span></span></span></span>个子集，在每个子集上抽取一个<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi><mi>o</mi><mi>o</mi><mi>t</mi><mi>s</mi><mi>t</mi><mi>r</mi><mi>a</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">bootstrap</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathdefault">b</span><span class="mord mathdefault">o</span><span class="mord mathdefault">o</span><span class="mord mathdefault">t</span><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span><span class="mord mathdefault" style="margin-right:.02778em">r</span><span class="mord mathdefault">a</span><span class="mord mathdefault">p</span></span></span></span>样本集，然后施以主分量变换。所有的主分量被用作分类器训练。他们的结果表示旋转森林比随机森林更准确。需要指出的是这里的“旋转”指的是特征空间上的广义的旋转，而与图像中的旋转没有任何联系。</p><p>Osman通过增量式的特征选择改进随机森林而提出了在线随机森林。与其它在线学习方法相似，特点在于其学习是随着训练数据的逐步增加而趋于完善，实验表明其表现随着数据的加入逼近离线随机森林，在所有数据全部获取的情况下达到离线随机森林的水平。</p><p>随机森林的一个另重要发展是语义纹元森林（Semantic Texton Forest, STF）。纹元森林方法首先将局部的小图块聚类，然后将它们组织成有层次的纹元树，最后用这些树对新的图块做分类。与经典的随机森林的最大差别在于它利用像素样本邻域上下文的信息。这通常是以扩展分割函数运算来实现的，纹元森林的分割函数不仅可以是基于本训练样本特征向量，还可以是其邻域内两个样本特征向量之间的运算。这一重要的扩展将纹理信息、邻域信息也考虑在内，大大增强了决策树对信息的分辨力。</p><h3 id="35-随机森林的代码实现"><a class="markdownIt-Anchor" href="#35-随机森林的代码实现"></a> 3.5 随机森林的代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomForest</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""随机森林</span></span><br><span class="line"><span class="string">    -----------</span></span><br><span class="line"><span class="string">    n_estimators: int</span></span><br><span class="line"><span class="string">        树的数量</span></span><br><span class="line"><span class="string">    max_features: int</span></span><br><span class="line"><span class="string">        每棵树选用数据集中的最大的特征数</span></span><br><span class="line"><span class="string">    min_samples_split: int</span></span><br><span class="line"><span class="string">        每棵树中最小的分割数，比如 min_samples_split = 2表示树切到还剩下两个数据集时就停止</span></span><br><span class="line"><span class="string">    min_gain: float</span></span><br><span class="line"><span class="string">        每棵树切到小于min_gain后停止</span></span><br><span class="line"><span class="string">    max_depth: int</span></span><br><span class="line"><span class="string">        每棵树的最大层数</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_estimators=<span class="number">100</span>, min_samples_split=<span class="number">2</span>, min_gain=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 max_depth=float<span class="params">(<span class="string">"inf"</span>)</span>, max_features=None)</span>:</span></span><br><span class="line"></span><br><span class="line">        self.n_estimators = n_estimators</span><br><span class="line">        self.min_samples_split = min_samples_split</span><br><span class="line">        self.min_gain = min_gain</span><br><span class="line">        self.max_depth = max_depth</span><br><span class="line">        self.max_features = max_features</span><br><span class="line"></span><br><span class="line">        self.trees = []</span><br><span class="line">        <span class="comment"># 建立森林(bulid forest)</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.n_estimators):</span><br><span class="line">            tree = ClassificationTree(min_samples_split=self.min_samples_split, min_impurity=self.min_gain,</span><br><span class="line">                                      max_depth=self.max_depth)</span><br><span class="line">            self.trees.append(tree)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, Y)</span>:</span></span><br><span class="line">        <span class="comment"># 训练，每棵树使用随机的数据集(bootstrap)和随机的特征</span></span><br><span class="line">        <span class="comment"># every tree use random data set(bootstrap) and random feature</span></span><br><span class="line">        sub_sets = self.get_bootstrap_data(X, Y)</span><br><span class="line">        n_features = X.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> self.max_features == <span class="literal">None</span>:</span><br><span class="line">            self.max_features = int(np.sqrt(n_features))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_estimators):</span><br><span class="line">            <span class="comment"># 生成随机的特征</span></span><br><span class="line">            <span class="comment"># get random feature</span></span><br><span class="line">            sub_X, sub_Y = sub_sets[i]</span><br><span class="line">            idx = np.random.choice(n_features, self.max_features, replace=<span class="literal">True</span>)</span><br><span class="line">            sub_X = sub_X[:, idx]</span><br><span class="line">            self.trees[i].fit(sub_X, sub_Y)</span><br><span class="line">            self.trees[i].feature_indices = idx</span><br><span class="line">            print(<span class="string">"tree"</span>, i, <span class="string">"fit complete"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        y_preds = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_estimators):</span><br><span class="line">            idx = self.trees[i].feature_indices</span><br><span class="line">            sub_X = X[:, idx]</span><br><span class="line">            y_pre = self.trees[i].predict(sub_X)</span><br><span class="line">            y_preds.append(y_pre)</span><br><span class="line">        y_preds = np.array(y_preds).T</span><br><span class="line">        y_pred = []</span><br><span class="line">        <span class="keyword">for</span> y_p <span class="keyword">in</span> y_preds:</span><br><span class="line">            <span class="comment"># np.bincount()可以统计每个索引出现的次数</span></span><br><span class="line">            <span class="comment"># np.argmax()可以返回数组中最大值的索引</span></span><br><span class="line">            <span class="comment"># cheak np.bincount() and np.argmax() in numpy Docs</span></span><br><span class="line">            y_pred.append(np.bincount(y_p.astype(<span class="string">'int'</span>)).argmax())</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_bootstrap_data</span><span class="params">(self, X, Y)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过bootstrap的方式获得n_estimators组数据</span></span><br><span class="line">        <span class="comment"># get int(n_estimators) datas by bootstrap</span></span><br><span class="line"></span><br><span class="line">        m = X.shape[<span class="number">0</span>]</span><br><span class="line">        Y = Y.reshape(m, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 合并X和Y，方便bootstrap (conbine X and Y)</span></span><br><span class="line">        X_Y = np.hstack((X, Y))</span><br><span class="line">        np.random.shuffle(X_Y)</span><br><span class="line"></span><br><span class="line">        data_sets = []</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.n_estimators):</span><br><span class="line">            idm = np.random.choice(m, m, replace=<span class="literal">True</span>)</span><br><span class="line">            bootstrap_X_Y = X_Y[idm, :]</span><br><span class="line">            bootstrap_X = bootstrap_X_Y[:, :<span class="number">-1</span>]</span><br><span class="line">            bootstrap_Y = bootstrap_X_Y[:, <span class="number">-1</span>:]</span><br><span class="line">            data_sets.append([bootstrap_X, bootstrap_Y])</span><br><span class="line">        <span class="keyword">return</span> data_sets</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 测试代码</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载digits数据集</span></span><br><span class="line">data = datasets.load_digits()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line"><span class="comment"># 切割训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.4</span>, seed=<span class="number">2</span>)</span><br><span class="line">print(<span class="string">"X_train.shape:"</span>, X_train.shape)</span><br><span class="line">print(<span class="string">"Y_train.shape:"</span>, y_train.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化随机森林</span></span><br><span class="line">clf = RandomForest(n_estimators=<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line">print(<span class="string">"Accuracy:"</span>, accuracy)</span><br></pre></td></tr></table></figure><h2 id="4-对比实验"><a class="markdownIt-Anchor" href="#4-对比实验"></a> 4 对比实验</h2><h3 id="41-实验结果"><a class="markdownIt-Anchor" href="#41-实验结果"></a> 4.1 实验结果</h3><p>为了相对公正地对比各模型地差别，采用<code>sklearn</code>库中的模型进行实验，同时所有的模型都采用默认的参数设置。数据集使用<code>sklearn</code>生成线性可分、环形形状、月亮形状三种不同数据分布数据集。</p><table><thead><tr><th style="text-align:center">模型</th><th style="text-align:center">平均精度</th><th style="text-align:center">线性可分精度</th><th style="text-align:center">环形形状精度</th><th style="text-align:center">月亮形状精度</th></tr></thead><tbody><tr><td style="text-align:center">KNN</td><td style="text-align:center">0.9067</td><td style="text-align:center">0.97</td><td style="text-align:center">0.80</td><td style="text-align:center">0.95</td></tr><tr><td style="text-align:center">Linear SVM</td><td style="text-align:center">0.7567</td><td style="text-align:center">1.00</td><td style="text-align:center">0.42</td><td style="text-align:center">0.85</td></tr><tr><td style="text-align:center">RBF SVM</td><td style="text-align:center">90.67</td><td style="text-align:center">0.97</td><td style="text-align:center">0.82</td><td style="text-align:center">0.93</td></tr><tr><td style="text-align:center">Decision Tree</td><td style="text-align:center">0.8333</td><td style="text-align:center">97</td><td style="text-align:center">0.75</td><td style="text-align:center">0.78</td></tr><tr><td style="text-align:center">RandomForest-10</td><td style="text-align:center">0.8933</td><td style="text-align:center">0.93</td><td style="text-align:center">0.82</td><td style="text-align:center">0.93</td></tr><tr><td style="text-align:center">RandomForest-25</td><td style="text-align:center">0.9000</td><td style="text-align:center">0.95</td><td style="text-align:center">0.82</td><td style="text-align:center">0.93</td></tr><tr><td style="text-align:center">RandomForest-50</td><td style="text-align:center">0.9000</td><td style="text-align:center">0.97</td><td style="text-align:center">0.80</td><td style="text-align:center">0.97</td></tr><tr><td style="text-align:center">RandomForest-100</td><td style="text-align:center">0.9067</td><td style="text-align:center">0.97</td><td style="text-align:center">0.82</td><td style="text-align:center">0.93</td></tr><tr><td style="text-align:center">AdaBoost</td><td style="text-align:center">0.8767</td><td style="text-align:center">0.95</td><td style="text-align:center">0.78</td><td style="text-align:center">0.90</td></tr><tr><td style="text-align:center">Navie Bayes</td><td style="text-align:center">0.8733</td><td style="text-align:center">1.00</td><td style="text-align:center">0.80</td><td style="text-align:center">0.82</td></tr></tbody></table><p><img src="https://note.youdao.com/yws/api/personal/file/2564C56395A84B8B9754C2AE503CEF95?method=download&amp;shareKey=8b554a6b2605c8e1efb19e259e852d13" alt="实验结果图"></p><p>从实验结果可看出，随机森林模型在三种不同数据分布的数据集上都保持有较高的精度。同时，与其他模型相比较，随机森林模型的精度毫不逊色。而且，随机森林模型在三种数据集上的精度比单一的决策树都提升了很多。但是，可以看出，随机森林模型随着内部基分类器（决策树）数目的增加，其精度并不会有较大的提高。</p><h3 id="42-实验代码"><a class="markdownIt-Anchor" href="#42-实验代码"></a> 4.2 实验代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons, make_circles, make_classification</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier, AdaBoostClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"> </span><br><span class="line">h = <span class="number">.02</span>  <span class="comment"># mesh的步长</span></span><br><span class="line"> </span><br><span class="line">names = [<span class="string">"Nearest Neighbors"</span>, <span class="string">"Linear SVM"</span>, <span class="string">"RBF SVM"</span>, <span class="string">"Decision Tree"</span>,</span><br><span class="line">         <span class="string">"Random Forest_10"</span>, <span class="string">"Random Forest_25"</span>,<span class="string">"Random Forest_50"</span>,<span class="string">"Random Forest_100"</span>,</span><br><span class="line">         <span class="string">"AdaBoost"</span>, <span class="string">"Naive Bayes"</span>,]</span><br><span class="line">classifiers = [</span><br><span class="line">    KNeighborsClassifier(<span class="number">3</span>),</span><br><span class="line">    SVC(kernel=<span class="string">"linear"</span>, C=<span class="number">0.025</span>),</span><br><span class="line">    SVC(gamma=<span class="number">2</span>, C=<span class="number">1</span>),</span><br><span class="line">    DecisionTreeClassifier(),</span><br><span class="line">    RandomForestClassifier(n_estimators=<span class="number">10</span>),</span><br><span class="line">    RandomForestClassifier(n_estimators=<span class="number">25</span>),</span><br><span class="line">    RandomForestClassifier(n_estimators=<span class="number">50</span>),</span><br><span class="line">    RandomForestClassifier(n_estimators=<span class="number">100</span>),</span><br><span class="line">    AdaBoostClassifier(),</span><br><span class="line">    GaussianNB(),]</span><br><span class="line"> </span><br><span class="line">X, y = make_classification(n_features=<span class="number">2</span>, n_redundant=<span class="number">0</span>, n_informative=<span class="number">2</span>,</span><br><span class="line">                           random_state=<span class="number">1</span>, n_clusters_per_class=<span class="number">1</span>)</span><br><span class="line">rng = np.random.RandomState(<span class="number">2</span>)</span><br><span class="line">X += <span class="number">2</span> * rng.uniform(size=X.shape)</span><br><span class="line">linearly_separable = (X, y)</span><br><span class="line"> </span><br><span class="line">datasets = [make_moons(noise=<span class="number">0.3</span>, random_state=<span class="number">0</span>),</span><br><span class="line">            make_circles(noise=<span class="number">0.2</span>, factor=<span class="number">0.5</span>, random_state=<span class="number">1</span>),</span><br><span class="line">            linearly_separable</span><br><span class="line">            ]</span><br><span class="line"> </span><br><span class="line">figure = plt.figure(figsize=(<span class="number">27</span>, <span class="number">9</span>))</span><br><span class="line">i = <span class="number">1</span></span><br><span class="line"><span class="comment"># 遍历每个数据集</span></span><br><span class="line"><span class="keyword">for</span> ds <span class="keyword">in</span> datasets:</span><br><span class="line">    <span class="comment"># 处理数据集</span></span><br><span class="line">    X, y = ds</span><br><span class="line">    X = StandardScaler().fit_transform(X)</span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">.4</span>)</span><br><span class="line"> </span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">.5</span>, X[:, <span class="number">0</span>].max() + <span class="number">.5</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">.5</span>, X[:, <span class="number">1</span>].max() + <span class="number">.5</span></span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class="line">                         np.arange(y_min, y_max, h))</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 先绘制数据集</span></span><br><span class="line">    cm = plt.cm.RdBu</span><br><span class="line">    cm_bright = ListedColormap([<span class="string">'#FF0000'</span>, <span class="string">'#0000FF'</span>])</span><br><span class="line">    ax = plt.subplot(len(datasets), len(classifiers) + <span class="number">1</span>, i)</span><br><span class="line">    <span class="comment"># 绘制训练集</span></span><br><span class="line">    ax.scatter(X_train[:, <span class="number">0</span>], X_train[:, <span class="number">1</span>], c=y_train, cmap=cm_bright)</span><br><span class="line">    <span class="comment"># 绘制测试集</span></span><br><span class="line">    ax.scatter(X_test[:, <span class="number">0</span>], X_test[:, <span class="number">1</span>], c=y_test, cmap=cm_bright, alpha=<span class="number">0.6</span>)</span><br><span class="line">    ax.set_xlim(xx.min(), xx.max())</span><br><span class="line">    ax.set_ylim(yy.min(), yy.max())</span><br><span class="line">    ax.set_xticks(())</span><br><span class="line">    ax.set_yticks(())</span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 遍历每个分类器</span></span><br><span class="line">    <span class="keyword">for</span> name, clf <span class="keyword">in</span> zip(names, classifiers):</span><br><span class="line">        ax = plt.subplot(len(datasets), len(classifiers) + <span class="number">1</span>, i)</span><br><span class="line">        clf.fit(X_train, y_train)</span><br><span class="line">        score = clf.score(X_test, y_test)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 绘制决策边界</span></span><br><span class="line">        <span class="keyword">if</span> hasattr(clf, <span class="string">"decision_function"</span>):</span><br><span class="line">            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, <span class="number">1</span>]</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 绘制结果</span></span><br><span class="line">        Z = Z.reshape(xx.shape)</span><br><span class="line">        ax.contourf(xx, yy, Z, cmap=cm, alpha=<span class="number">.8</span>)</span><br><span class="line"> </span><br><span class="line">        ax.scatter(X_train[:, <span class="number">0</span>], X_train[:, <span class="number">1</span>], c=y_train, cmap=cm_bright)</span><br><span class="line">        ax.scatter(X_test[:, <span class="number">0</span>], X_test[:, <span class="number">1</span>], c=y_test, cmap=cm_bright,</span><br><span class="line">                   alpha=<span class="number">0.6</span>)</span><br><span class="line"> </span><br><span class="line">        ax.set_xlim(xx.min(), xx.max())</span><br><span class="line">        ax.set_ylim(yy.min(), yy.max())</span><br><span class="line">        ax.set_xticks(())</span><br><span class="line">        ax.set_yticks(())</span><br><span class="line">        ax.set_title(name)</span><br><span class="line">        ax.text(xx.max() - <span class="number">.3</span>, yy.min() + <span class="number">.3</span>, (<span class="string">'%.2f'</span> % score).lstrip(<span class="string">'0'</span>),</span><br><span class="line">                size=<span class="number">15</span>, horizontalalignment=<span class="string">'right'</span>)</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line"> </span><br><span class="line">figure.subplots_adjust(left=<span class="number">.02</span>, right=<span class="number">.98</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a class="markdownIt-Anchor" href="#参考文献"></a> 参考文献</h2><ul><li>雷震. 随机森林及其在遥感影像处理中应用研究[D].上海交通大学,2012.</li><li>周志华. 机器学习[M]. 北京:清华大学出版社, 2016:73-93.</li><li><a href="https://zhuanlan.zhihu.com/p/32179140" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32179140</a></li><li><a href="https://zhuanlan.zhihu.com/p/32180057" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32180057</a></li></ul></div><div class="article-footer"><blockquote class="mt-2x"><ul class="post-copyright list-unstyled"><li class="post-copyright-link hidden-xs"><strong>本文链接：</strong> <a href="https://hengxincheung.github.io/%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%80%E8%BF%B0/" title="随机森林简述" target="_blank" rel="external">https://hengxincheung.github.io/%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%80%E8%BF%B0/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！</li></ul></blockquote><div class="panel panel-default panel-badger"><div class="panel-body"><figure class="media"><div class="media-left"><a href="https://github.com/hengxincheung" target="_blank" class="img-burn thumb-sm visible-lg"><img src="/images/avatar.jpg" class="img-rounded w-full" alt=""></a></div><div class="media-body"><h3 class="media-heading"><a href="https://github.com/hengxincheung" target="_blank"><span class="text-dark">hengxincheung</span><small class="ml-1x">码农</small></a></h3><div>Encode my life, Decode the future</div></div></figure></div></div></div></article><section id="comments"><div id="vcomments"></div></section></div><nav class="bar bar-footer clearfix" data-stick-bottom><div class="bar-inner"><ul class="pager pull-left"><li class="prev"><a href="/Java%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/07-%E5%A4%8D%E7%94%A8%E7%B1%BB/" title="07-复用类"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a></li><li class="next"><a href="/git/git%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8/" title="git命令大全"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a></li><li class="toggle-toc"><a class="toggle-btn collapsed" data-toggle="collapse" href="#collapseToc" aria-expanded="false" title="文章目录" role="button"><span>[&nbsp;</span><span>文章目录</span> <i class="text-collapsed icon icon-anchor"></i> <i class="text-in icon icon-close"></i> <span>]</span></a></li></ul><button type="button" class="btn btn-fancy btn-donate pop-onhover bg-gradient-warning" data-toggle="modal" data-target="#donateModal"><span>赏</span></button><div class="bar-right"><div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div></div></div></nav><div class="modal modal-center modal-small modal-xs-full fade" id="donateModal" tabindex="-1" role="dialog"><div class="modal-dialog" role="document"><div class="modal-content donate"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button><div class="modal-body"><div class="donate-box"><div class="donate-head"><p>感谢您的支持，我会继续努力的!</p></div><div class="tab-content"><div role="tabpanel" class="tab-pane fade active in" id="alipay"><div class="donate-payimg"><img src="/images/donate/alipay.jpg" alt="扫码支持" title="扫一扫"></div><p class="text-muted mv">扫码打赏，你说多少就多少</p><p class="text-grey">打开支付宝扫一扫，即可进行扫码打赏哦</p></div><div role="tabpanel" class="tab-pane fade" id="wechatpay"><div class="donate-payimg"><img src="/images/donate/wechatpay.jpg" alt="扫码支持" title="扫一扫"></div><p class="text-muted mv">扫码打赏，你说多少就多少</p><p class="text-grey">打开微信扫一扫，即可进行扫码打赏哦</p></div></div><div class="donate-footer"><ul class="nav nav-tabs nav-justified" role="tablist"><li role="presentation" class="active"><a href="#alipay" id="alipay-tab" role="tab" data-toggle="tab" aria-controls="alipay" aria-expanded="true"><i class="icon icon-alipay"></i> 支付宝</a></li><li role="presentation"><a href="#wechatpay" role="tab" id="wechatpay-tab" data-toggle="tab" aria-controls="wechatpay" aria-expanded="false"><i class="icon icon-wepay"></i> 微信支付</a></li></ul></div></div></div></div></div></div></main><footer class="footer" itemscope itemtype="http://schema.org/WPFooter"><ul class="social-links"><li><a href="https://github.com/hengxincheung" target="_blank" title="Github" data-toggle="tooltip" data-placement="top"><i class="icon icon-github"></i></a></li><li><a href="/null" target="_blank" title="Weibo" data-toggle="tooltip" data-placement="top"><i class="icon icon-weibo"></i></a></li><li><a href="/null" target="_blank" title="Twitter" data-toggle="tooltip" data-placement="top"><i class="icon icon-twitter"></i></a></li><li><a href="/null" target="_blank" title="Behance" data-toggle="tooltip" data-placement="top"><i class="icon icon-behance"></i></a></li><li><a href="/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul><div class="copyright"><div class="publishby"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span></div></div></footer><script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script><script>window.jQuery||document.write('<script src="js/jquery.min.js"><\/script>')</script><script src="/js/plugin.min.js"></script><script src="/js/application.js"></script><script>!function(T){var N={TRANSLATION:{POSTS:"文章",PAGES:"页面",CATEGORIES:"分类",TAGS:"标签",UNTITLED:"(未命名)"},ROOT_URL:"/",CONTENT_URL:"/content.json"};T.INSIGHT_CONFIG=N}(window)</script><script src="/js/insight.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//cdn.jsdelivr.net/npm/valine"></script><script type="text/javascript">var GUEST=["nick","mail","link"],meta="nick,mail,link";meta=meta.split(",").filter(function(e){return GUEST.indexOf(e)>-1}),new Valine({el:"#vcomments",verify:!1,notify:!1,appId:"",appKey:"",placeholder:"Just go go",avatar:"mm",meta:meta,pageSize:"10",visitor:!1})</script><script src="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.3.5/dist/jquery.fancybox.min.js"></script><script>$(document).ready(function(){$("article img").not("[hidden]").not(".panel-body img").each(function(){var a=$(this),t=a.attr("alt"),n=a.parent("a");if(n.length<1){var e=this.getAttribute("src"),r=e.lastIndexOf("?");-1!=r&&(e=e.substring(0,r)),n=a.wrap('<a href="'+e+'"></a>').parent("a")}n.attr("data-fancybox","images"),t&&n.attr("data-caption",t)}),$().fancybox({selector:'[data-fancybox="images"]',hash:!1,loop:!1})})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script></body></html><!-- rebuild by neat -->